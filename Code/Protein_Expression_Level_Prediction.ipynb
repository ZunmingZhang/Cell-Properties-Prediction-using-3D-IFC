{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn.utils import weight_norm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os,sys\n",
    "import copy\n",
    "from math import ceil\n",
    "import math\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        weight_norm(nn.Conv3d(in_channels, out_channels, 3, padding=1)),\n",
    "        nn.ReLU(inplace=True),\n",
    "#         weight_norm(nn.Conv3d(out_channels, out_channels, 3, padding=1)),\n",
    "#         nn.ReLU(inplace=True)\n",
    "    )   \n",
    "    \n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class, in_channels,dropout=0.2,filters=[16,32,64,128], latent_channels=512):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(in_channels, filters[0])\n",
    "        self.dconv_down2 = double_conv(filters[0], filters[1])\n",
    "        self.dconv_down3 = double_conv(filters[1], filters[2])\n",
    "        self.dconv_down4 = double_conv(filters[2], filters[3])\n",
    "#         self.dconv_down5 = double_conv(filters[3], filters[4])\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)   \n",
    "        \n",
    "#         self.dconv_up4 = double_conv(filters[4], filters[3])\n",
    "#         self.dconv_up3 = double_conv(filters[3], filters[2])\n",
    "        self.dconv_up2 = double_conv(filters[2], filters[1])\n",
    "        self.dconv_up1 = double_conv(filters[1], filters[0])\n",
    "        \n",
    "        self.bn3d_1 = nn.BatchNorm3d(filters[0])\n",
    "        self.bn3d_2 = nn.BatchNorm3d(filters[1])\n",
    "        self.bn3d_3 = nn.BatchNorm3d(filters[2])\n",
    "        self.bn3d_4 = nn.BatchNorm3d(filters[3])\n",
    "#         self.bn3d_5 = nn.BatchNorm3d(filters[4])\n",
    "        \n",
    "        self.bn3d_1_1 = nn.BatchNorm3d(filters[0])\n",
    "        self.bn3d_2_1 = nn.BatchNorm3d(filters[1])\n",
    "#         self.bn3d_3_1 = nn.BatchNorm3d(filters[2])\n",
    "        \n",
    "        \n",
    "        self.conv_last = nn.Conv3d(filters[0], in_channels, 1)\n",
    "        self.fc1 = nn.Linear(filters[3]*20*20*20,latent_channels)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(latent_channels)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(latent_channels, n_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.relu_out1 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dconv_down1(x) #16 80^3\n",
    "        x = self.maxpool(x) #16 40^3\n",
    "        x = self.bn3d_1(x)\n",
    "        \n",
    "        x = self.dconv_down2(x) #32 40^3\n",
    "        x = self.maxpool(x) #32 20^3\n",
    "        x = self.bn3d_2(x)\n",
    "        \n",
    "        x = self.dconv_down3(x) #64 20^3\n",
    "        x = self.maxpool(x)    #64 10^3\n",
    "        x = self.bn3d_3(x)\n",
    "        \n",
    "#         x = self.dconv_down4(x) #128 10^3\n",
    "#         x = self.maxpool(x)     #128 5^3\n",
    "#         x = self.bn3d_4(x)\n",
    "        \n",
    "        #Classifier\n",
    "        x1 = self.dconv_down4(x)  #256 5^3\n",
    "#         x1 = self.maxpool(x1) #256 2^3\n",
    "        x1 = self.bn3d_4(x1)\n",
    "        \n",
    "        flatten = x1.view(x1.size(0),-1)\n",
    "        y_SSC = self.fc1(flatten)\n",
    "        y_SSC = self.bn_fc1(y_SSC)\n",
    "        y_SSC = self.relu(y_SSC)\n",
    "        y_SSC = self.dropout1(y_SSC)\n",
    "        \n",
    "        \n",
    "        y = y_SSC\n",
    "        y = self.out(y)\n",
    "        prediction = nn.functional.log_softmax(y,dim=1)\n",
    "    \n",
    "#         x = self.upsample(x) #256 10^3\n",
    "#         x = self.dconv_up4(x) #128 10^3\n",
    "        \n",
    "#         x = self.upsample(x)     #128 10^3    \n",
    "#         x = self.dconv_up3(x) #64 10^3\n",
    "#         x = self.bn3d_3_1(x)\n",
    "        \n",
    "        x = self.upsample(x) #64 20^3    \n",
    "        x = self.dconv_up2(x) #32 20^3\n",
    "        x = self.bn3d_2_1(x)\n",
    "        \n",
    "        x = self.upsample(x)  #32 40^3       \n",
    "        x = self.dconv_up1(x) #16 40^3\n",
    "        x = self.bn3d_1_1(x)\n",
    "        \n",
    "        x = self.upsample(x)  #16 80^3 \n",
    "        x = self.conv_last(x) #1 80^3\n",
    "        \n",
    "        out = self.relu_out1(x)\n",
    "        \n",
    "        return out, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_model_summary import summary\n",
    "print(summary(VAE(2,1),torch.zeros((1,1,160, 160, 160)), show_input=False, show_hierarchical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "# print(m)\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        m.weight.data.normal_(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          my_dpi=100,\n",
    "                          title=None,\n",
    "                          saved=True,\n",
    "                          save_name='Unsupervised Learning.png'):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.1%}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.tile(np.sum(cf,axis = 0),(3,))]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "#         accuracy  = np.trace(cf) / 3\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nBalanced Accuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize, dpi=my_dpi)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if saved:\n",
    "        plt.savefig(save_name, dpi=my_dpi*10, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "#     print(output.size())\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def cal_consistency_weight(epoch, init_ep=0, end_ep=150, init_w=0.0, end_w=20.0):\n",
    "    \"\"\"Sets the weights for the consistency loss\"\"\"\n",
    "    if epoch > end_ep:\n",
    "        weight_cl = end_w\n",
    "    elif epoch < init_ep:\n",
    "        weight_cl = init_w\n",
    "    else:\n",
    "        T = float(epoch - init_ep)/float(end_ep - init_ep)\n",
    "        #weight_mse = T * (end_w - init_w) + init_w #linear\n",
    "        weight_cl = (math.exp(-5.0 * (1.0 - T) * (1.0 - T))) * (end_w - init_w) + init_w #exp\n",
    "    #print('Consistency weight: %f'%weight_cl)\n",
    "    return weight_cl\n",
    "\n",
    "def update_ema_variables(model, model_teacher, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1.0 - 1.0 / float(global_step + 1), alpha)\n",
    "    for param_t, param in zip(model_teacher.parameters(), model.parameters()):\n",
    "        param_t.data.mul_(alpha).add_(1 - alpha, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(out, prediction, target, original, MSE_weight=0.5):\n",
    "    CE = F.cross_entropy(prediction, target)\n",
    "#     BCE = F.binary_cross_entropy_with_logits(prediction, target)\n",
    "#     out = torch.sigmoid(out)\n",
    "    MSE = F.mse_loss(out*65535, original*65535)\n",
    "    \n",
    "    loss = MSE * MSE_weight + CE * (1 - MSE_weight)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sup(label_loader, model, device, criterions, optimizers, epoch, args, k=2):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    MSE_weight = args.MSE_weight\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    criterion_ce, criterion_mse = criterions\n",
    "    optimizer1, optimizer2 = optimizers\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    LabelList = torch.tensor([1]).to(device)\n",
    "    PredList = torch.tensor([1]).to(device)\n",
    "    epoch_samples = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    label_iter = iter(label_loader) \n",
    "    for i in range(len(label_iter)):\n",
    "        inputs, target= next(label_iter)\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        sl = inputs.shape\n",
    "        batch_size = sl[0]\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        epoch_samples += batch_size\n",
    "        \n",
    "        # compute output\n",
    "        output, predictions = model(inputs)\n",
    "        loss_ce = criterion_ce(predictions, target)\n",
    "        loss_mse = criterion_mse(output*255, inputs*255)\n",
    "        loss = loss_mse * MSE_weight + loss_ce * (1 - MSE_weight)\n",
    "#         if epoch<20:\n",
    "#             loss = criterion_mse(output*255, inputs*255)\n",
    "#         else:\n",
    "#             loss_ce = criterion_ce(predictions, target)\n",
    "#             loss_mse = criterion_mse(output*255, inputs*255)\n",
    "#             loss = loss_mse * MSE_weight + loss_ce * (1 - MSE_weight)\n",
    "        \n",
    "        _, preds = torch.max(predictions, 1)\n",
    "        LabelList = torch.cat([LabelList, target.view(-1)], dim=0)\n",
    "        PredList = torch.cat([PredList, preds.view(-1)], dim=0)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        \n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(predictions.data, target, topk=(1, k))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        if epoch < 5:\n",
    "            optimizer1.zero_grad()\n",
    "        else:\n",
    "            optimizer2.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if epoch < 5:\n",
    "            optimizer1.step()\n",
    "        else:\n",
    "            optimizer2.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec @ 1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec @ {k} {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(label_iter), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, k=k, top5=top5))\n",
    "    epoch_acc_balanced = balanced_accuracy_score(LabelList[1:].cpu(), PredList[1:].cpu())\n",
    "    epoch_acc = running_corrects.double() / epoch_samples\n",
    "    print('\\n\\nEpoch: {0}\\t Balanced Accuracy: {1}\\t Running Accuracy: {2}\\n\\n'.format(epoch, epoch_acc_balanced, epoch_acc))\n",
    "    return top1.avg , losses.avg, epoch_acc_balanced, epoch_acc\n",
    "\n",
    "\n",
    "def validate(val_loader, model, device, criterions, args, mode='valid', k=2, weight_pi=20):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    MSE_weight = args.MSE_weight\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    criterion_ce, criterion_mse = criterions\n",
    "    \n",
    "    LabelList = torch.tensor([1]).to(device)\n",
    "    PredList = torch.tensor([1]).to(device)\n",
    "    epoch_samples = 0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target) in enumerate(val_loader):\n",
    "            sl = inputs.shape\n",
    "            batch_size = sl[0]\n",
    "            target = target.to(device)\n",
    "            inputs = inputs.to(device)\n",
    "            epoch_samples += batch_size\n",
    "            \n",
    "            # compute output\n",
    "            output, predictions = model(inputs)\n",
    "        \n",
    "            loss_ce = criterion_ce(predictions, target)\n",
    "            loss_mse = criterion_mse(output*255, inputs*255)\n",
    "            loss = loss_mse * MSE_weight + loss_ce * (1 - MSE_weight) # * weight_pi\n",
    "            \n",
    "            _, preds = torch.max(predictions, 1)\n",
    "            LabelList = torch.cat([LabelList, target.view(-1)], dim=0)\n",
    "            PredList = torch.cat([PredList, preds.view(-1)], dim=0)\n",
    "            running_corrects += torch.sum(preds == target.data)\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(predictions.data, target, topk=(1, k))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    " \n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    " \n",
    "            if i % args.print_freq == 0:\n",
    "                if mode == 'test':\n",
    "                    print('Test: [{0}/{1}]\\t'\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                          'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                          'Prec @ 1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                          'Prec @ {k} {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                           i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                           top1=top1, top5=top5, k=k))\n",
    "                else:\n",
    "                    print('Valid: [{0}/{1}]\\t'\n",
    "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                          'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                          'Prec @ 1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                          'Prec @ {k} {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                           i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                           top1=top1, top5=top5, k=k))\n",
    "    epoch_acc_balanced = balanced_accuracy_score(LabelList[1:].cpu(), PredList[1:].cpu())\n",
    "    epoch_acc = running_corrects.double() / epoch_samples\n",
    "    report = classification_report(LabelList[1:].cpu(), PredList[1:].cpu(), labels=[0,1])\n",
    "    confusionmatrix = confusion_matrix(LabelList[1:].cpu(), PredList[1:].cpu(), labels=[0,1])\n",
    "    print(' ****** Prec @ 1 {top1.avg:.3f} Prec @ {k} {top5.avg:.3f} Loss {loss.avg:.3f} '\n",
    "          .format(top1=top1, top5=top5, loss=losses, k=k))\n",
    "    print('\\n\\nBalanced Accuracy: {0}\\t Running Accuracy: {1}\\n\\n'.format(epoch_acc_balanced, epoch_acc))\n",
    "\n",
    "    return top1.avg, losses.avg, epoch_acc_balanced, epoch_acc, report, confusionmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar', dirname='.'):\n",
    "    fpath = os.path.join(dirname, filename + '_latest.pth.tar')\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        bpath = os.path.join(dirname, filename + '_best.pth.tar')\n",
    "        shutil.copyfile(fpath, bpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_model(model, dataloaders, optimizers, schedulers, criterions, args, device):\n",
    "    best_acc_balanced = 0\n",
    "    best_test_acc_balanced = 0\n",
    "    prec1s_tr, acc1_tr_balanced, acc1_tr, losses_tr = [], [], [], []\n",
    "    losses_cl_tr = []\n",
    "    prec1s_val, acc1_val_balanced, acc1_val, losses_val, losses_et_val = [], [], [], [], []\n",
    "    prec1s_t_tr = []\n",
    "    prec1s_t_val, acc1_t_val_balanced, acc1_t_val = [], [], []\n",
    "    learning_rate, weights_cl = [], []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            if args.model=='mt': model_teacher.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    " \n",
    "        \n",
    "    ckpt_dir = args.ckpt+'_'+args.arch+'_'+args.model+'_'+args.optim+'_ul%.3f'%(args.unlabel_percent)\n",
    "    ckpt_dir = ckpt_dir + '_e%d'%(args.epochs)\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    print(ckpt_dir)\n",
    "\n",
    "    batch_size_label=args.batch_size\n",
    "\n",
    "    label_loader, val_loader = dataloaders\n",
    "    \n",
    "    print(\"Batch size (label): \", batch_size_label)\n",
    "    \n",
    "    optimizer1, optimizer2 = optimizers\n",
    "    scheduler1, scheduler2 = schedulers\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        epoch_starttime = time.time()\n",
    "        if epoch < 5:\n",
    "            for param_group in optimizer1.param_groups:\n",
    "                print(\"LR:\", param_group['lr'])\n",
    "                lr = param_group['lr']\n",
    "        else:\n",
    "            for param_group in optimizer2.param_groups:\n",
    "                print(\"LR:\", param_group['lr'])\n",
    "                lr = param_group['lr']\n",
    "        \n",
    "        # train for one epoch\n",
    "        if args.model == 'baseline':\n",
    "            print('Supervised Training')\n",
    "            prec1_tr, loss_tr, train_acc_balanced, train_acc = train_sup(label_loader, model, device, criterions, optimizers, epoch, args)\n",
    "        \n",
    "        # evaluate on validation set        \n",
    "        prec1_val, loss_val, val_acc_balanced, val_acc, val_report, val_confusionmatrix = validate(val_loader, model, device, criterions, args, mode='valid')\n",
    "        \n",
    "        # learning scheduler\n",
    "        if epoch < 5:\n",
    "            scheduler1.step(loss_val)\n",
    "        elif epoch > args.epochs * (1/3):\n",
    "            scheduler2.step(loss_val)\n",
    "        \n",
    "        # append values\n",
    "        acc1_tr_balanced.append(train_acc_balanced)\n",
    "        acc1_tr.append(train_acc)\n",
    "        acc1_val_balanced.append(val_acc_balanced)\n",
    "        acc1_val.append(val_acc)\n",
    "        prec1s_tr.append(prec1_tr)\n",
    "        losses_tr.append(loss_tr)\n",
    "        prec1s_val.append(prec1_val)\n",
    "        losses_val.append(loss_val)\n",
    "\n",
    "        learning_rate.append(lr)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = val_acc_balanced > best_acc_balanced\n",
    "        if is_best:\n",
    "            best_test_acc_balanced = val_acc_balanced\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            val_report_best = val_report\n",
    "            val_confusionmatrix_best = val_confusionmatrix\n",
    "        print(\"Best test balanced accuracy: %.3f\"%best_test_acc_balanced)\n",
    "        best_acc_balanced = max(val_acc_balanced, best_acc_balanced)\n",
    "        dict_checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc_balanced': best_acc_balanced,\n",
    "            'best_test_acc_balanced' : best_test_acc_balanced,\n",
    "            'prec1s_tr': prec1s_tr,\n",
    "            'acc1_tr': acc1_tr,\n",
    "            'acc1_tr_balanced': acc1_tr_balanced,\n",
    "            'losses_tr': losses_tr,\n",
    "            'losses_cl_tr': losses_cl_tr,\n",
    "            'prec1s_val': prec1s_val,\n",
    "            'acc1_val': acc1_val,\n",
    "            'acc1_val_balanced':acc1_val_balanced,\n",
    "            'losses_val': losses_val,\n",
    "            'learning_rate' : learning_rate,\n",
    "            'val_report': val_report, \n",
    "            'val_confusionmatrix': val_confusionmatrix,\n",
    "        }\n",
    "        \n",
    "        save_checkpoint(dict_checkpoint, is_best, str(args.boundary), dirname=ckpt_dir)\n",
    "        print('\\nEpoch time:', time.time()-epoch_starttime, 's\\n')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc1_tr_balanced, acc1_val_balanced, acc1_tr, acc1_val, losses_tr, losses_val, learning_rate, val_report_best, val_confusionmatrix_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def datapreparation(Datadir, fold):\n",
    "    allfold = ['Fold1', 'Fold2', 'Fold3', 'Fold4', 'Fold5']\n",
    "    valfold = allfold.pop(fold)\n",
    "    trainpath = []\n",
    "    valpath = []\n",
    "    \n",
    "    fpath = []\n",
    "    for trainfold in allfold:\n",
    "        traindir = Datadir + \"/High/\"+trainfold+\"/\"\n",
    "        for dirpath, dirnames, filenames in os.walk(traindir):\n",
    "            for filename in [f for f in filenames if f.endswith(\".mat\")]:\n",
    "                tempfpath =os.path.join(dirpath, filename)\n",
    "                fpath.append(tempfpath)\n",
    "    datalength = list(range(0,len(fpath)))\n",
    "    train = list(range(0,len(fpath)))\n",
    "    print('High Train data length: %d' %(len(train)))\n",
    "    for i in train:\n",
    "        trainpath.append(fpath[i])\n",
    "        \n",
    "    fpath = []\n",
    "    for trainfold in allfold:\n",
    "        traindir = Datadir + \"/Low1/\"+trainfold+\"/\"\n",
    "        for dirpath, dirnames, filenames in os.walk(traindir):\n",
    "            for filename in [f for f in filenames if f.endswith(\".mat\")]:\n",
    "                tempfpath =os.path.join(dirpath, filename)\n",
    "                fpath.append(tempfpath)\n",
    "    datalength = list(range(0,len(fpath)))\n",
    "    train = list(range(0,len(fpath)))\n",
    "    print('Low Train data length: %d' %(len(train)))\n",
    "    for i in train:\n",
    "        trainpath.append(fpath[i])\n",
    "        \n",
    "    fpath = []\n",
    "    traindir = Datadir + \"/High/\"+valfold+\"/\"\n",
    "    for dirpath, dirnames, filenames in os.walk(traindir):\n",
    "        for filename in [f for f in filenames if f.endswith(\".mat\")]:\n",
    "            tempfpath =os.path.join(dirpath, filename)\n",
    "            fpath.append(tempfpath)\n",
    "    datalength = list(range(0,len(fpath)))\n",
    "    val = list(range(0,len(fpath)))\n",
    "    print('Val Train data length: %d' %(len(val)))\n",
    "    for i in val:\n",
    "        valpath.append(fpath[i])\n",
    "        \n",
    "    fpath = []\n",
    "    traindir = Datadir + \"/Low1/\"+valfold+\"/\"\n",
    "    for dirpath, dirnames, filenames in os.walk(traindir):\n",
    "        for filename in [f for f in filenames if f.endswith(\".mat\")]:\n",
    "            tempfpath =os.path.join(dirpath, filename)\n",
    "            fpath.append(tempfpath)\n",
    "    datalength = list(range(0,len(fpath)))\n",
    "    val = list(range(0,len(fpath)))\n",
    "    print('Val Train data length: %d' %(len(val)))\n",
    "    for i in val:\n",
    "        valpath.append(fpath[i])\n",
    "        \n",
    "    return trainpath,valpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, paths, transforms=None):\n",
    "        self.paths = paths\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = sio.loadmat(self.paths[index])['data']\n",
    "        data[data<3000]=0\n",
    "        x = torch.from_numpy(data.astype(np.float32)/65535)\n",
    "        x = x.unsqueeze(dim = 0)\n",
    "        x = x.unsqueeze(dim = 0)\n",
    "        if 'Low' in self.paths[index]:\n",
    "            label = int(0)\n",
    "        else:\n",
    "            label = int(1)\n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "#         print(x.shape)\n",
    "        x = x.squeeze(dim = 0)\n",
    "#         data = sio.loadmat(self.paths[index])['data']\n",
    "#         data = data.astype(np.float32)/65535\n",
    "#         data = np.transpose(data, (2,0,1))\n",
    "#         x = torch.from_numpy(data)\n",
    "#         x = torch.unsqueeze(x,0)\n",
    "#         x = torch.unsqueeze(x,0)\n",
    "#         print(x.shape)\n",
    "#         if 'Low' in self.paths[index]:\n",
    "#             label = int(0)\n",
    "#         else:\n",
    "#             label = int(1)\n",
    "            \n",
    "#         if self.transforms:\n",
    "#             x = F.interpolate(x, (400,400,400), mode='bicubic', align_corners=True)\n",
    "#         x = torch.squeeze(x,0)\n",
    "        return x,label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import argparse\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import date\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Semi-supervised learning Training')\n",
    "parser.add_argument('--model', metavar='MODEL', default='baseline', help='model: (default: baseline)', choices=['baseline', 'pi', 'mt'])\n",
    "parser.add_argument('--optim', '-o', metavar='OPTIM', default='adam', help='optimizer: '+' (default: adam)', choices=['adam', 'sgd'])\n",
    "parser.add_argument('--arch', metavar='ARCH', default='VAE', help='architecture: (default: VAE)', choices=['VAE', 'UNet'])\n",
    "parser.add_argument('--epochs', default=150, type=int, help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--batch_size', default=2, type=int, help='mini-batch size (default: 32)')\n",
    "parser.add_argument('--lr', default=5e-5, type=float, help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "parser.add_argument('--weight-decay', default=1e-4, type=float, help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('--weight_l1', default=1e-3, type=float, help='l1 regularization (default: 1e-3)')\n",
    "parser.add_argument('--print_freq', default=800, type=int, help='print frequency (default: 100)')\n",
    "parser.add_argument('--resume', default='', type=str, help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--num_classes',default=3, type=int, help='number of classes in the model')\n",
    "parser.add_argument('--ckpt', default='ckpt', type=str, help='path to save checkpoint (default: ckpt)')\n",
    "parser.add_argument('--boundary',default=0, type=int, help='different label/unlabel division [0,9]')\n",
    "parser.add_argument('--gpu',default=0, type=str, help='cuda_visible_devices')\n",
    "parser.add_argument('--weight-pi', default=5, type=float, help='weight pi (default: 1)')\n",
    "parser.add_argument('--weight-mt', default=8, type=float, help='weight mt (default: 8)')\n",
    "parser.add_argument('--MSE_weight', default=0.5, type=float, help='MSe weight (default: 0.1)')\n",
    "\n",
    "\n",
    "parser.add_argument('--unlabel_percent', default=0, type=float, help='unlabel percent (default: 0.95)')\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(args)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu)\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "# Data preparation - Straight KFold\n",
    "Datadir = \"../Data_Augumented\"\n",
    "args.ckpt = 'ckpt_'+ str(date.today())\n",
    "# runs = 0\n",
    "for runs in range(0,5):\n",
    "    print(\"Fold-\",str(runs),\": Initializing Datasets and Dataloaders...\")\n",
    "    \n",
    "    Fold = runs\n",
    "    args.boundary = runs\n",
    "    \n",
    "    trainpath,valpath = datapreparation(Datadir,Fold)\n",
    "    print('label train data vol.: ',len(trainpath))\n",
    "    print('val data vol.: ',len(valpath))\n",
    "    \n",
    "    transforms_data = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True) \n",
    "    label_dataset = MyDataset(trainpath, transforms=transforms_data)\n",
    "    val_dataset = MyDataset(valpath, transforms=transforms_data)\n",
    "    label_loader = torch.utils.data.DataLoader(label_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True, pin_memory=True)\n",
    "    dataloaders = (label_loader, val_loader)\n",
    "    \n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    \n",
    "    num_class = 2\n",
    "    in_channel = 1\n",
    "#     num_epochs = 100\n",
    "    \n",
    "    if args.arch == 'VAE':\n",
    "        model = VAE(num_class, in_channel)\n",
    "    elif args.arch == 'UNet':\n",
    "        model = UNet(num_class, in_channel)\n",
    "    else:\n",
    "        raise('Architecture not implented')\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer_ft1 = optim.Adam(model.parameters(), lr=args.lr,betas = (0.9, 0.999),eps=1e-08,weight_decay=0)\n",
    "    optimizer_ft2 = optim.Adam(model.parameters(), lr=1e-2,betas = (0.9, 0.999),eps=1e-08,weight_decay=0)\n",
    "    exp_lr_scheduler1 = lr_scheduler.ReduceLROnPlateau(optimizer_ft1, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=1, min_lr=1e-7, eps=1e-08)\n",
    "    exp_lr_scheduler2 = lr_scheduler.ReduceLROnPlateau(optimizer_ft2, mode='min', factor=0.5, patience=5, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=1, min_lr=1e-7, eps=1e-08)       \n",
    "    optimizers = (optimizer_ft1, optimizer_ft2)\n",
    "    schedulers = (exp_lr_scheduler1, exp_lr_scheduler2)\n",
    "    criterion_ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "    criterion_mse = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    criterions = (criterion_ce, criterion_mse)\n",
    "    \n",
    "    model,acc1_tr_balanced,acc1_val_balanced,acc1_tr,acc1_val,losses_tr,losses_val,learning_rate,val_report_best,val_confusionmatrix_best = train_model(model, dataloaders, optimizers, schedulers, criterions, args, device)\n",
    "    \n",
    "    ckpt_dir = args.ckpt+'_'+args.arch+'_'+args.model+'_'+args.optim+'_ul%.3f'%(args.unlabel_percent)\n",
    "    ckpt_dir = ckpt_dir + '_e%d'%(args.epochs)\n",
    "    \n",
    "    if args.model == 'baseline':\n",
    "        savebasename = 'Baseline_' + str(date.today())\n",
    "    fname1 = savebasename+'_'+str(runs)+'Loss'+'.png'\n",
    "    fname2 = savebasename+'_'+str(runs)+'BA'+'.png'\n",
    "    fname3 = savebasename+'_'+str(runs)+'LR'+'.png'\n",
    "    fname4 = savebasename+'_'+str(runs)+'Val_CM'+'.png'\n",
    "    \n",
    "    my_dpi = 200\n",
    "    \n",
    "    ## Loss figure\n",
    "    plt.figure(figsize=(5, 5), dpi=my_dpi)\n",
    "    plt.semilogy(range(0,args.epochs), losses_tr,label='Training Loss')\n",
    "    plt.semilogy(range(0,args.epochs), losses_val,label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.savefig(os.path.join(ckpt_dir, fname1), dpi=my_dpi * 10)\n",
    "    plt.show()\n",
    "    \n",
    "    ## Balanced Accuracy figure\n",
    "    plt.figure(figsize=(5, 5), dpi=my_dpi)\n",
    "    plt.plot(range(0,args.epochs), acc1_tr_balanced,label='Training Accuracy')\n",
    "    plt.plot(range(0,args.epochs), acc1_val_balanced,label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.savefig(os.path.join(ckpt_dir, fname2), dpi=my_dpi * 10)\n",
    "    plt.show()\n",
    "    \n",
    "    ## Learning Rate figure\n",
    "    plt.figure(figsize=(5, 5), dpi=my_dpi)\n",
    "    plt.plot(range(0,args.epochs), learning_rate)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.savefig(os.path.join(ckpt_dir, fname3), dpi=my_dpi * 10)\n",
    "    plt.show()\n",
    "    \n",
    "    ## Validation confusion matrix\n",
    "    confusionMat = np.asarray(val_confusionmatrix_best)\n",
    "    sumconfusion = np.sum(confusionMat,axis = 1).T\n",
    "    summat = np.tile(sumconfusion,(num_class,1)).T\n",
    "    percentconfusion_val = np.divide(confusionMat,summat)\n",
    "    \n",
    "    print('Validation Report: \\n', val_report_best)\n",
    "    categories = ['Low','High']\n",
    "    make_confusion_matrix(percentconfusion_val, \n",
    "                          #group_names=labels,\n",
    "                          categories=categories,\n",
    "                          percent=False,\n",
    "                          cbar=False,\n",
    "                          figsize=(4 ,4),\n",
    "                          cmap='Greens',my_dpi=200,title = 'Supervised Learning',\n",
    "                          saved=True, save_name=os.path.join(ckpt_dir, fname4))\n",
    "\n",
    "    del label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
